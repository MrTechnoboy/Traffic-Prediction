{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============= GPU SETUP =============\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "if device.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ],
   "id": "262bb4c3c95c9201"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============= DATA =============\n",
    "num_nodes = 9\n",
    "\n",
    "edges = [\n",
    "    [0, 1], [1, 0], [1, 2], [2, 1],\n",
    "    [3, 4], [4, 3], [4, 5], [5, 4],\n",
    "    [6, 7], [7, 6], [7, 8], [8, 7],\n",
    "    [0, 3], [3, 0], [3, 6], [6, 3],\n",
    "    [1, 4], [4, 1], [4, 7], [7, 4],\n",
    "    [2, 5], [5, 2], [5, 8], [8, 5],\n",
    "]\n",
    "\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().to(device)  # Move to GPU\n",
    "\n",
    "np.random.seed(42)\n",
    "node_features = torch.tensor([\n",
    "    [8.0, 1, 45.0, 12, 1, 0.3],\n",
    "    [8.0, 1, 25.0, 35, 1, 0.7],\n",
    "    [8.0, 1, 50.0, 8, 0, 0.2],\n",
    "    [8.0, 1, 40.0, 15, 1, 0.4],\n",
    "    [8.0, 1, 20.0, 45, 1, 0.8],\n",
    "    [8.0, 1, 35.0, 20, 1, 0.5],\n",
    "    [8.0, 1, 30.0, 28, 0, 0.6],\n",
    "    [8.0, 1, 38.0, 18, 1, 0.45],\n",
    "    [8.0, 1, 55.0, 5, 0, 0.15],\n",
    "], dtype=torch.float).to(device)  # Move to GPU\n",
    "\n",
    "labels = torch.tensor([0, 2, 0, 1, 2, 1, 2, 1, 0], dtype=torch.long).to(device)  # Move to GPU"
   ],
   "id": "57455e1b05028337"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============= GAT LAYER =============\n",
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.6, alpha=0.2, concat=True):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2 * out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = torch.mm(x, self.W)\n",
    "        num_nodes = h.size(0)\n",
    "\n",
    "        source_nodes = edge_index[0]\n",
    "        target_nodes = edge_index[1]\n",
    "\n",
    "        h_concat = torch.cat([h[source_nodes], h[target_nodes]], dim=1)\n",
    "        e = self.leakyrelu(torch.matmul(h_concat, self.a))\n",
    "\n",
    "        attention = torch.zeros(num_nodes, num_nodes, device=x.device)  # Keep on same device\n",
    "        attention[source_nodes, target_nodes] = e.squeeze()\n",
    "        attention = F.softmax(attention, dim=0)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "\n",
    "        h_prime = torch.matmul(attention.t(), h)\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "\n",
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_heads, dropout=0.6, alpha=0.2, concat=True):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat = concat\n",
    "\n",
    "        self.attention_heads = nn.ModuleList([\n",
    "            GATLayer(in_features, out_features, dropout, alpha, concat)\n",
    "            for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        head_outputs = [head(x, edge_index) for head in self.attention_heads]\n",
    "\n",
    "        if self.concat:\n",
    "            return torch.cat(head_outputs, dim=1)\n",
    "        else:\n",
    "            return torch.mean(torch.stack(head_outputs), dim=0)\n",
    "\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, in_features, hidden_dim, num_classes, num_heads=4, dropout=0.6):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.gat1 = MultiHeadGATLayer(\n",
    "            in_features,\n",
    "            hidden_dim,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            concat=True\n",
    "        )\n",
    "\n",
    "        self.gat2 = GATLayer(\n",
    "            hidden_dim * num_heads,\n",
    "            num_classes,\n",
    "            dropout=dropout,\n",
    "            concat=False\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gat1(x, edge_index)\n",
    "        x = self.gat2(x, edge_index)\n",
    "        return x"
   ],
   "id": "89d8da96afa937b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============= TRAINING =============\n",
    "model = GAT(in_features=6, hidden_dim=8, num_classes=3, num_heads=4, dropout=0.3).to(device)  # Move to GPU\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=5e-4)\n",
    "\n",
    "print(f'\\nModel parameters on: {next(model.parameters()).device}')\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(node_features, edge_index)\n",
    "    loss = F.cross_entropy(out, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(node_features, edge_index)\n",
    "        pred = out.argmax(dim=1)\n",
    "        correct = (pred == labels).sum().item()\n",
    "        acc = correct / len(labels)\n",
    "    return acc, pred\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining GAT on GPU...\")\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    if epoch % 20 == 0:\n",
    "        acc, _ = test()\n",
    "        print(f'Epoch {epoch:03d}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')\n",
    "\n",
    "# Final predictions\n",
    "acc, predictions = test()\n",
    "print(f'\\n=== Final Results ===')\n",
    "print(f'Accuracy: {acc:.4f}')\n",
    "print(f'True labels:      {labels.cpu().numpy()}')  # Move to CPU for display\n",
    "print(f'Predictions:      {predictions.cpu().numpy()}')\n",
    "print(f'\\n0=Low, 1=Medium, 2=High Congestion')"
   ],
   "id": "3281095be6015f03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ============= VISUALIZE =============\n",
    "def visualize_attention():\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = node_features\n",
    "        h = torch.mm(x, model.gat1.attention_heads[0].W)\n",
    "\n",
    "        source_nodes = edge_index[0]\n",
    "        target_nodes = edge_index[1]\n",
    "\n",
    "        h_concat = torch.cat([h[source_nodes], h[target_nodes]], dim=1)\n",
    "        e = model.gat1.attention_heads[0].leakyrelu(\n",
    "            torch.matmul(h_concat, model.gat1.attention_heads[0].a)\n",
    "        )\n",
    "\n",
    "        num_nodes = node_features.size(0)\n",
    "        attention = torch.zeros(num_nodes, num_nodes, device=device)\n",
    "        attention[source_nodes, target_nodes] = e.squeeze()\n",
    "        attention = F.softmax(attention, dim=0)\n",
    "\n",
    "    # Move to CPU for visualization\n",
    "    attention_cpu = attention.cpu().numpy()\n",
    "    predictions_cpu = predictions.cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(attention_cpu, cmap='hot', interpolation='nearest')\n",
    "    plt.colorbar(label='Attention Weight')\n",
    "    plt.title('Attention Weight Matrix')\n",
    "    plt.xlabel('Source Node')\n",
    "    plt.ylabel('Target Node')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    G = nx.Graph()\n",
    "    for i in range(num_nodes):\n",
    "        G.add_node(i)\n",
    "    for i in range(0, len(edges), 2):\n",
    "        G.add_edge(edges[i][0], edges[i][1])\n",
    "\n",
    "    pos = {\n",
    "        0: (0, 2), 1: (1, 2), 2: (2, 2),\n",
    "        3: (0, 1), 4: (1, 1), 5: (2, 1),\n",
    "        6: (0, 0), 7: (1, 0), 8: (2, 0)\n",
    "    }\n",
    "\n",
    "    colors = ['green', 'yellow', 'red']\n",
    "    node_colors = [colors[predictions_cpu[i]] for i in range(num_nodes)]\n",
    "\n",
    "    nx.draw(G, pos, node_color=node_colors, node_size=1200,\n",
    "            with_labels=True, font_size=14, font_weight='bold',\n",
    "            edge_color='gray', width=2)\n",
    "    plt.title('GAT Predictions\\nGreen=Low, Yellow=Med, Red=High')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_attention()"
   ],
   "id": "e33dff7fb617e679"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
